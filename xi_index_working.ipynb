{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # de compositores restantes:  19\n"
     ]
    }
   ],
   "source": [
    "#dataframe datos de compositores \n",
    "\n",
    "datos_composers = {}\n",
    "carpeta = r'Sequences\\labels'\n",
    "archivos_en_carpeta = os.listdir(carpeta)\n",
    "index0 = 0\n",
    "indice = 0\n",
    "\n",
    "for archivo in archivos_en_carpeta:\n",
    "    ruta_completa = os.path.join(carpeta, archivo)\n",
    "    serie = pd.read_csv(ruta_completa, header = None)\n",
    "    composer = archivo.split('-')[1].capitalize() # nombre compositor\n",
    "    datos_composers[composer] = {} #genero bibio para composer\n",
    "    datos_composers[composer]['Birth_year'] = archivo.split('-')[0] #año de nacimiento\n",
    "    index1 = serie.iloc[0, 0].split('\\t')[0] #el # del primer serie del composer\n",
    "    index2 = int(serie.iloc[len(serie)-3, 0].split('\\t')[0]) - index0 # # Piezas\n",
    "    index0 = index2 + index0 # numero total de piezas anteriores\n",
    "    datos_composers[composer]['# Piezas'] = index2 # Piezas\n",
    "    datos_composers[composer]['Indice'] = indice\n",
    "    indice += 1\n",
    "\n",
    "composers = {}\n",
    "M = 0\n",
    "carpeta = r'Sequences\\Series'\n",
    "archivos_en_carpeta = os.listdir(carpeta)\n",
    "\n",
    "for archivo in archivos_en_carpeta:\n",
    "    ruta_completa = os.path.join(carpeta, archivo)\n",
    "    serie = pd.read_csv(ruta_completa)\n",
    "    # escoge una serie\n",
    "    composer = archivo.split('-')[1].capitalize() # nombre compositor\n",
    "    composers[composer] = {}\n",
    "\n",
    "    for pieza in range( datos_composers[composer]['# Piezas'] ):\n",
    "        N = serie.iloc[0, 0].split('\\t')[1] # # de elementos por pieza\n",
    "        M = int(N) + M\n",
    "        index_n1 = 0 \n",
    "        index_n2 = int(N)+2 \n",
    "        serie_n = serie[index_n1 + 2:index_n2].reset_index(drop=True) # resetear index\n",
    "        serie = serie[index_n2 +1:] # recortar serie Original\n",
    "        serie_n.index += 1 # que index empiece desde 1\n",
    "        num_serie_T = serie.columns[0]  # numero de serie de todo el dataset\n",
    "        num_serie = pieza + 1\n",
    "        composers[composer]['Serie_'+str(num_serie)] = serie_n.squeeze().to_numpy().astype(float) # agregamos pieza al dicc composer con key como # serie\n",
    "\n",
    "import copy\n",
    "def midi_to_hz(midi_notes):\n",
    "    # Fórmula para convertir número MIDI a Hz\n",
    "    freqs = 440.0 * 2 ** ((midi_notes - 69) / 12.0)\n",
    "    return freqs\n",
    "\n",
    "# composers_interp = copy.deepcopy(composers)\n",
    "composers_Hz = copy.deepcopy(composers)\n",
    "\n",
    "# for composer in composers_Hz.keys():\n",
    "#     for pieza in composers_Hz[composer].keys():\n",
    "#         subject = composers[composer][pieza]\n",
    "#         composers_Hz[composer][pieza] = midi_to_hz(subject)\n",
    "\n",
    "composers_Hz_depurado = copy.deepcopy(composers_Hz)\n",
    "datos_composers_Hz_depurado = copy.deepcopy(datos_composers)\n",
    "\n",
    "for i,composer in enumerate(composers_Hz.keys()):\n",
    "    d = 0\n",
    "    for pieza in composers_Hz[composer].keys():\n",
    "        if len(composers_Hz[composer][pieza])//2 < 400:\n",
    "            del composers_Hz_depurado[composer][pieza]\n",
    "            d = d + 1\n",
    "    datos_composers_Hz_depurado[composer]['# Piezas'] = datos_composers[composer]['# Piezas'] - d\n",
    "\n",
    "\n",
    "# 40 promedio de numero de piezas por compositor\n",
    "composers_Hz_depurado_v2 = copy.deepcopy(composers_Hz_depurado)\n",
    "composers_Hz_depurado_v2_keychange = copy.deepcopy(composers_Hz_depurado_v2)\n",
    "datos_composers_Hz_depurado_v2 = copy.deepcopy(datos_composers_Hz_depurado)\n",
    "\n",
    "for composer in composers.keys():\n",
    "    if datos_composers_Hz_depurado[composer]['# Piezas'] < 30:\n",
    "        del composers_Hz_depurado_v2[composer]\n",
    "        del datos_composers_Hz_depurado_v2[composer]\n",
    "    \n",
    "for i,composer in enumerate(composers_Hz_depurado_v2.keys()):\n",
    "    datos_composers_Hz_depurado_v2[composer]['Indice'] = i \n",
    "\n",
    "for composer in composers_Hz_depurado_v2.keys():\n",
    "    for i,serie in enumerate(composers_Hz_depurado_v2[composer].keys()):\n",
    "        composers_Hz_depurado_v2_keychange[composer]['Serie_' + str(i+1)] = composers_Hz_depurado_v2_keychange[composer].pop(serie)\n",
    "\n",
    "print(\" # de compositores restantes: \", len(composers_Hz_depurado_v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fathon\n",
    "from fathon import fathonUtils as fu\n",
    "\n",
    "def DFA_logbin(time_series):\n",
    "    # Convertir la serie temporal a un formato compatible con Fathon\n",
    "    my_data = fu.toAggregated(time_series)\n",
    "    data_length = len(my_data)\n",
    "\n",
    "    # Definir tamaños de ventana\n",
    "    winSizes = fu.linRangeByStep(20, min(500, data_length // 4))  # Ajusta los límites según los datos\n",
    "    revSeg = False  # Si deseas usar segmentos invertidos\n",
    "    polOrd = 2  # Orden del polinomio para eliminar tendencias\n",
    "\n",
    "    # Crear el objeto DFA y calcular las fluctuaciones\n",
    "    pydfa = fathon.DFA(my_data)\n",
    "    n, F = pydfa.computeFlucVec(winSizes, revSeg=revSeg, polOrd=polOrd)\n",
    "\n",
    "    # Aplicar log-binning\n",
    "    log_bins = np.logspace(np.log10(min(n)), np.log10(max(n)), num=15)\n",
    "    binned_F = []\n",
    "    binned_n = []\n",
    "    for i in range(len(log_bins) - 1):\n",
    "        mask = (n >= log_bins[i]) & (n < log_bins[i + 1])\n",
    "        if np.any(mask):\n",
    "            binned_n.append(np.mean(n[mask]))\n",
    "            binned_F.append(np.mean(F[mask]))\n",
    "\n",
    "    return binned_n, binned_F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_increment_series(time_series):\n",
    "    \"\"\"Calcula la serie de incrementos.\"\"\"\n",
    "    return np.diff(time_series)\n",
    "\n",
    "def decompose_series(increment_series):\n",
    "    \"\"\"Descompone la serie de incrementos en magnitudes y signos, y resta las medias.\"\"\"\n",
    "    magnitude_series = np.abs(increment_series)\n",
    "    sign_series = np.sign(increment_series)\n",
    "    \n",
    "    # Restar las medias\n",
    "    magnitude_series -= np.mean(magnitude_series)\n",
    "    sign_series -= np.mean(sign_series)\n",
    "    \n",
    "    return magnitude_series, sign_series\n",
    "\n",
    "def integrate_series(series):\n",
    "    \"\"\"Integra la serie para asegurar que esté positivamente correlacionada.\"\"\"\n",
    "    return np.cumsum(series)\n",
    "\n",
    "\n",
    "def mdfa(time_series):\n",
    "    \"\"\"\n",
    "    Implementación del MDFA siguiendo los pasos especificados.\n",
    "    :param time_series: Serie temporal (numpy array).\n",
    "    :param scale_range: Rango de tamaños de ventana para el análisis de escala.\n",
    "    :return: Exponentes de escala para la serie de magnitudes y signos.\n",
    "    \"\"\"\n",
    "    # Paso 1: Calcular la serie de incrementos\n",
    "    increment_series = calculate_increment_series(time_series)\n",
    "    \n",
    "    # Paso 2: Descomponer la serie de incrementos en magnitud y signo, restar medias\n",
    "    magnitude_series, sign_series = decompose_series(increment_series)\n",
    "    \n",
    "    # Paso 3: Integrar las series de magnitudes y signos\n",
    "    integrated_magnitude = integrate_series(magnitude_series)\n",
    "    integrated_sign = integrate_series(sign_series)\n",
    "    \n",
    "    # Paso 4: Aplicar DFA a ambas series integradas\n",
    "    n_mag, F_mag = DFA_logbin(integrated_magnitude)\n",
    "    # alpha_sign = DFA(integrated_sign)\n",
    "\n",
    "    \n",
    "    return n_mag, F_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "\"\"\"\n",
    "iaaft - Iterative amplitude adjusted Fourier transform surrogates\n",
    "\n",
    "        This module implements the IAAFT method [1] to generate time series\n",
    "        surrogates (i.e. randomized copies of the original time series) which\n",
    "        ensures that each randomised copy preserves the power spectrum of the\n",
    "        original time series.\n",
    "\n",
    "[1] Venema, V., Ament, F. & Simmer, C. A stochastic iterative amplitude\n",
    "    adjusted Fourier Transform algorithm with improved accuracy (2006), Nonlin.\n",
    "    Proc. Geophys. 13, pp. 321--328\n",
    "    https://doi.org/10.5194/npg-13-321-2006\n",
    "\n",
    "\"\"\"\n",
    "# Created: Tue Jun 22, 2021  09:44am\n",
    "# Last modified: Tue Jun 22, 2021  12:39pm\n",
    "#\n",
    "# Copyright (C) 2021  Bedartha Goswami <bedartha.goswami@uni-tuebingen.de> This\n",
    "# program is free software: you can redistribute it and/or modify it under the\n",
    "# terms of the GNU Affero General Public License as published by the Free\n",
    "# Software Foundation, either version 3 of the License, or (at your option) any\n",
    "# later version.\n",
    "\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU Affero General Public License for more details.\n",
    "\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def iaaft(x, ns, tol_pc=5., verbose=True, maxiter=1E6, sorttype=\"quicksort\"):\n",
    "    \"\"\"\n",
    "    Returns iAAFT surrogates of given time series.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    x : numpy.ndarray, with shape (N,)\n",
    "        Input time series for which IAAFT surrogates are to be estimated.\n",
    "    ns : int\n",
    "        Number of surrogates to be generated.\n",
    "    tol_pc : float\n",
    "        Tolerance (in percent) level which decides the extent to which the\n",
    "        difference in the power spectrum of the surrogates to the original\n",
    "        power spectrum is allowed (default = 5).\n",
    "    verbose : bool\n",
    "        Show progress bar (default = `True`).\n",
    "    maxiter : int\n",
    "        Maximum number of iterations before which the algorithm should\n",
    "        converge. If the algorithm does not converge until this iteration\n",
    "        number is reached, the while loop breaks.\n",
    "    sorttype : string\n",
    "        Type of sorting algorithm to be used when the amplitudes of the newly\n",
    "        generated surrogate are to be adjusted to the original data. This\n",
    "        argument is passed on to `numpy.argsort`. Options include: 'quicksort',\n",
    "        'mergesort', 'heapsort', 'stable'. See `numpy.argsort` for further\n",
    "        information. Note that although quick sort can be a bit faster than \n",
    "        merge sort or heap sort, it can, depending on the data, have worse case\n",
    "        spends that are much slower.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xs : numpy.ndarray, with shape (ns, N)\n",
    "        Array containing the IAAFT surrogates of `x` such that each row of `xs`\n",
    "        is an individual surrogate time series.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    numpy.argsort\n",
    "\n",
    "    \"\"\"\n",
    "    # as per the steps given in Lancaster et al., Phys. Rep (2018)\n",
    "    nx = x.shape[0]\n",
    "    xs = np.zeros((ns, nx))\n",
    "    maxiter = 10000\n",
    "    ii = np.arange(nx)\n",
    "\n",
    "    # get the fft of the original array\n",
    "    x_amp = np.abs(np.fft.fft(x))\n",
    "    x_srt = np.sort(x)\n",
    "    r_orig = np.argsort(x)\n",
    "\n",
    "    # loop over surrogate number\n",
    "    pb_fmt = \"{desc:<5.5}{percentage:3.0f}%|{bar:30}{r_bar}\"\n",
    "    pb_desc = \"Estimating IAAFT surrogates ...\"\n",
    "    for k in tqdm(range(ns), bar_format=pb_fmt, desc=pb_desc,\n",
    "                  disable=not verbose):\n",
    "\n",
    "        # 1) Generate random shuffle of the data\n",
    "        count = 0\n",
    "        r_prev = np.random.permutation(ii)\n",
    "        r_curr = r_orig\n",
    "        z_n = x[r_prev]\n",
    "        percent_unequal = 100.\n",
    "\n",
    "        # core iterative loop\n",
    "        while (percent_unequal > tol_pc) and (count < maxiter):\n",
    "            r_prev = r_curr\n",
    "\n",
    "            # 2) FFT current iteration yk, and then invert it but while\n",
    "            # replacing the amplitudes with the original amplitudes but\n",
    "            # keeping the angles from the FFT-ed version of the random\n",
    "            y_prev = z_n\n",
    "            fft_prev = np.fft.fft(y_prev)\n",
    "            phi_prev = np.angle(fft_prev)\n",
    "            e_i_phi = np.exp(phi_prev * 1j)\n",
    "            z_n = np.fft.ifft(x_amp * e_i_phi)\n",
    "\n",
    "            # 3) rescale zk to the original distribution of x\n",
    "            r_curr = np.argsort(z_n, kind=sorttype)\n",
    "            z_n[r_curr] = x_srt.copy()\n",
    "            percent_unequal = ((r_curr != r_prev).sum() * 100.) / nx\n",
    "\n",
    "            # 4) repeat until number of unequal entries between r_curr and \n",
    "            # r_prev is less than tol_pc percent\n",
    "            count += 1\n",
    "\n",
    "        if count >= (maxiter - 1):\n",
    "            print(\"maximum number of iterations reached!\")\n",
    "\n",
    "        xs[k] = np.real(z_n)\n",
    "\n",
    "    return xs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ajuste_polinomial(x, y, grado):\n",
    "    \"\"\"\n",
    "    Ajusta un polinomio de grado especificado a los datos y grafica el ajuste.\n",
    "    \n",
    "    :param x: Array de valores x de los datos.\n",
    "    :param y: Array de valores y de los datos.\n",
    "    :param grado: Grado del polinomio a ajustar.\n",
    "    :return: Coeficientes del polinomio ajustado.\n",
    "    \"\"\"\n",
    "    # Ajustar el polinomio usando np.polyfit\n",
    "    coeficientes = np.polyfit(x, y, grado)\n",
    "    \n",
    "    # Crear un polinomio a partir de los coeficientes ajustados\n",
    "    polinomio = np.poly1d(coeficientes)\n",
    "    \n",
    "    # Evaluar el polinomio ajustado en los puntos x\n",
    "    y_ajustado = polinomio(x)\n",
    "    \n",
    "    # Retornar los coeficientes del polinomio\n",
    "    return y_ajustado, coeficientes\n",
    "\n",
    "def evaluar_derivada(coeficientes, x_valor):\n",
    "    \"\"\"\n",
    "    Calcula la primera derivada de un polinomio y la evalúa en un valor arbitrario.\n",
    "    \n",
    "    :param coeficientes: Coeficientes del polinomio (array).\n",
    "    :param x_valor: Valor en el que se desea evaluar la derivada.\n",
    "    :return: Valor de la primera derivada del polinomio evaluada en x_valor.\n",
    "    \"\"\"\n",
    "    # Crear el polinomio a partir de los coeficientes\n",
    "    polinomio = np.poly1d(coeficientes)\n",
    "    \n",
    "    # Calcular la primera derivada del polinomio\n",
    "    derivada = np.polyder(polinomio)\n",
    "    \n",
    "    # Evaluar la derivada en el valor especificado\n",
    "    derivada_evaluada = derivada(x_valor)\n",
    "    return np.array(derivada_evaluada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import piecewise_regression\n",
    "\n",
    "# CARGAR DATOS\n",
    "\n",
    "def main(time_series, method):\n",
    "    H = []\n",
    "    # CALCULAR MDFA Ó DFA\n",
    "    if method == 'MDFA':\n",
    "        n, F = mdfa(time_series)\n",
    "    if method == 'DFA':\n",
    "        n, F = DFA_logbin(time_series)\n",
    "\n",
    "\n",
    "    # GENERAR SURROGADOS\n",
    "    flucts_surrogates = []\n",
    "    N = 100\n",
    "    surrogates = iaaft(time_series, N)\n",
    "    for i in range(N):\n",
    "        if method == 'MDFA':\n",
    "            n_surr, flucts_surr = mdfa(surrogates[i,:])\n",
    "        if method == 'DFA':\n",
    "            n_surr, flucts_surr = DFA_logbin(surrogates[i,:])\n",
    "        flucts_surrogates.append(np.log10(flucts_surr))\n",
    "    flucts_surrogates = np.vstack(flucts_surrogates)\n",
    "\n",
    "    # AJUSTAR REGRESIÓN LINEAL SEGMENTADA\n",
    "    n_breakpoints = 1\n",
    "    while True: \n",
    "        pw_fit = piecewise_regression.Fit(np.log10(n), np.log10(F), n_breakpoints=n_breakpoints)\n",
    "        pw_results = pw_fit.get_results()\n",
    "        pw_estimates = pw_results[\"estimates\"]\n",
    "\n",
    "        if pw_results['converged']:\n",
    "            print('converged')\n",
    "            break\n",
    "        elif n_breakpoints != 1:\n",
    "            n_breakpoints= n_breakpoints-1\n",
    "        else: \n",
    "            break\n",
    "    for value in pw_estimates:\n",
    "        if 'alpha' in value:\n",
    "            H.append(pw_estimates[value]['estimate'])\n",
    "\n",
    "    # # GRAFICAR\n",
    "\n",
    "    pw_fit.plot_data(color=\"red\", s=1,label='log(F(s))') # logF(s)\n",
    "    pw_fit.plot_breakpoints() #Breakpoints\n",
    "\n",
    "    lower_bound = np.min(flucts_surrogates, axis=0) # area sombreada surr\n",
    "    upper_bound = np.max(flucts_surrogates, axis=0) # area sombreada surr\n",
    "    plt.fill_between(np.log10(n_surr), lower_bound, upper_bound, color='blue', alpha=0.2, label='Surrogates')\n",
    "    \n",
    "    # AJUSTAR POLINOMIO CON GRADO # DE BREAKPOINTS\n",
    "    y_ajustado, coeficientes= ajuste_polinomial(np.log10(n), np.log10(F),grado=4)\n",
    "    # yˆ(xi)\n",
    "    derivadas = evaluar_derivada(coeficientes, np.log10(n))\n",
    "\n",
    "    # Derivadas para surrogados\n",
    "    derivadas_surr = np.zeros([N,len(np.log10(n_surr))])\n",
    "    for surr_index in range(N):\n",
    "        y_ajustado_surr, coeficientes_surr = ajuste_polinomial(np.log10(n_surr), flucts_surrogates[surr_index],grado=4)\n",
    "        derivadas_surr[surr_index,:] = evaluar_derivada(coeficientes_surr, np.log10(n_surr))\n",
    "    derivadas_surr_mean = np.mean(derivadas_surr, axis=0)\n",
    "    derivadas_surr_std = np.std(derivadas_surr, axis= 0)\n",
    "\n",
    "    # GrAFICAR POLINOMIO\n",
    "    plt.plot(np.log10(n), y_ajustado, color='green', label=f'Ajuste polinomial (grado {n_breakpoints+1})')\n",
    "\n",
    "    plt.xlabel('log(s)', fontsize=14)\n",
    "    plt.ylabel('log(F(s))', fontsize=14)\n",
    "    plt.title('DFA', fontsize=14)\n",
    "    plt.legend(loc=0, fontsize=7)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.log10(n), derivadas, color='green', ls='',marker='.',label='Derivadas de la serie original')\n",
    "    plt.fill_between(\n",
    "        np.log10(n_surr), \n",
    "        derivadas_surr_mean - derivadas_surr_std, \n",
    "        derivadas_surr_mean + derivadas_surr_std, \n",
    "        color='blue', alpha=0.2, label='Surrogates: Mean ± Std'\n",
    "    )\n",
    "    plt.xlabel('log(s)', fontsize=14)\n",
    "    plt.ylabel(\"Derivada del polinomio\", fontsize=14)\n",
    "    plt.title(\"Derivadas del ajuste polinomial vs log(s)\", fontsize=14)\n",
    "    plt.legend(loc=0, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "    # CALCULAR INDICE NO LINEALIDAD\n",
    "    E = np.sum(np.abs(derivadas - derivadas_surr_mean) / derivadas_surr_std)\n",
    "    E_index = E/len(derivadas)\n",
    "    return E_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for composer in composers_Hz_depurado_v2.keys():\n",
    "    xi_index = np.zeros((datos_composers_Hz_depurado_v2[composer]['# Piezas'], 2))\n",
    "    for i, serie in enumerate(composers_Hz_depurado_v2[composer].keys()): \n",
    "        # if int(serie.split('_')[1]) < 413:\n",
    "        #     print(serie)\n",
    "        #     continue\n",
    "        subject = composers_Hz_depurado_v2[composer][serie]\n",
    "        xi_index[i,0] = int(serie.split('_')[1])\n",
    "        xi_index[i,1] = main(subject, 'MDFA')\n",
    "    np.save('xi_index1/'+str(datos_composers_Hz_depurado_v2[composer]['Birth_year'])+'_'+str(composer)+'_xi.npy', xi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fathon\n",
    "from fathon import fathonUtils as fu\n",
    "\n",
    "def DFA(time_series):\n",
    "\n",
    "    # Cargar tus datos en lugar de los generados aleatoriamente\n",
    "    # Reemplaza esta línea con la carga de tus datos reales\n",
    "    my_data = time_series  # Reemplaza esta línea con tus datos reales\n",
    "\n",
    "    # Convertir la serie a un formato compatible con Fathon\n",
    "    my_data = fu.toAggregated(my_data)\n",
    "\n",
    "    # Determinar la longitud de la serie temporal\n",
    "    data_length = len(my_data)\n",
    "\n",
    "    # Ajustar los tamaños de ventana para que no excedan la longitud de los datos\n",
    "    winSizes = fu.linRangeByStep(20, min(500, data_length // 4))  # Ajusta según la longitud de tus datos\n",
    "    revSeg = False  # Configura si deseas usar segmentos invertidos\n",
    "    polOrd = 2 # Orden del polinomio para eliminar la tendencia, ajusta según tu cas3\n",
    "\n",
    "    # Crear el objeto DFA con tus datos\n",
    "    pydfa = fathon.DFA(my_data)\n",
    "\n",
    "    # Calcular las fluctuaciones\n",
    "    n, F = pydfa.computeFlucVec(winSizes, revSeg=revSeg, polOrd=polOrd)\n",
    "\n",
    "\n",
    "    # # Si deseas realizar ajustes múltiples en diferentes rangos, configura los límites aquí\n",
    "    # limits_list = np.array([[20, min(50, data_length // 4)]], dtype=int)  # Ajusta según sea necesario\n",
    "    # list_H, list_H_intercept = pydfa.multiFitFlucVec(limits_list)\n",
    "\n",
    "    # # Graficar los resultados de los múltiples ajustes\n",
    "    # clrs = ['k', 'b', 'm', 'c', 'y']\n",
    "    # stls = ['-', '--', '.-']\n",
    "\n",
    "    return n, F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdfa(time_series):\n",
    "    \"\"\"\n",
    "    Implementación del MDFA siguiendo los pasos especificados.\n",
    "    :param time_series: Serie temporal (numpy array).\n",
    "    :param scale_range: Rango de tamaños de ventana para el análisis de escala.\n",
    "    :return: Exponentes de escala para la serie de magnitudes y signos.\n",
    "    \"\"\"\n",
    "    # Paso 1: Calcular la serie de incrementos\n",
    "    increment_series = calculate_increment_series(time_series)\n",
    "    \n",
    "    # Paso 2: Descomponer la serie de incrementos en magnitud y signo, restar medias\n",
    "    magnitude_series, sign_series = decompose_series(increment_series)\n",
    "    \n",
    "    # Paso 3: Integrar las series de magnitudes y signos\n",
    "    integrated_magnitude = integrate_series(magnitude_series)\n",
    "    integrated_sign = integrate_series(sign_series)\n",
    "    \n",
    "    # Paso 4: Aplicar DFA a ambas series integradas\n",
    "    n_mag, F_mag = DFA(integrated_magnitude)\n",
    "    # alpha_sign = DFA(integrated_sign)\n",
    "\n",
    "    \n",
    "    return n_mag, F_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import piecewise_regression\n",
    "\n",
    "# CARGAR DATOS\n",
    "\n",
    "def main(time_series, method):\n",
    "    H = []\n",
    "    # CALCULAR MDFA Ó DFA\n",
    "    if method == 'MDFA':\n",
    "        n, F = mdfa(time_series)\n",
    "    if method == 'DFA':\n",
    "        n, F = DFA(time_series)\n",
    "\n",
    "\n",
    "    # GENERAR SURROGADOS\n",
    "    flucts_surrogates = []\n",
    "    N = 100\n",
    "    surrogates = iaaft(time_series, N)\n",
    "    for i in range(N):\n",
    "        if method == 'MDFA':\n",
    "            n_surr, flucts_surr = mdfa(surrogates[i,:])\n",
    "        if method == 'DFA':\n",
    "            n_surr, flucts_surr = DFA(surrogates[i,:])\n",
    "        flucts_surrogates.append(np.log10(flucts_surr))\n",
    "    flucts_surrogates = np.vstack(flucts_surrogates)\n",
    "\n",
    "    # AJUSTAR REGRESIÓN LINEAL SEGMENTADA\n",
    "    n_breakpoints = 1\n",
    "    while True: \n",
    "        pw_fit = piecewise_regression.Fit(np.log10(n), np.log10(F), n_breakpoints=n_breakpoints)\n",
    "        pw_results = pw_fit.get_results()\n",
    "        pw_estimates = pw_results[\"estimates\"]\n",
    "\n",
    "        if pw_results['converged']:\n",
    "            print('converged')\n",
    "            break\n",
    "        elif n_breakpoints != 1:\n",
    "            n_breakpoints= n_breakpoints-1\n",
    "        else: \n",
    "            break\n",
    "    for value in pw_estimates:\n",
    "        if 'alpha' in value:\n",
    "            H.append(pw_estimates[value]['estimate'])\n",
    "\n",
    "    # # GRAFICAR\n",
    "\n",
    "    pw_fit.plot_data(color=\"red\", s=1,label='log(F(s))') # logF(s)\n",
    "    pw_fit.plot_breakpoints() #Breakpoints\n",
    "\n",
    "    lower_bound = np.min(flucts_surrogates, axis=0) # area sombreada surr\n",
    "    upper_bound = np.max(flucts_surrogates, axis=0) # area sombreada surr\n",
    "    plt.fill_between(np.log10(n_surr), lower_bound, upper_bound, color='blue', alpha=0.2, label='Surrogates')\n",
    "    \n",
    "    # AJUSTAR POLINOMIO CON GRADO # DE BREAKPOINTS\n",
    "    past_idx = 0\n",
    "    derivadas = np.array([])\n",
    "    y_ajustados = np.array([])\n",
    "    for i in range(n_breakpoints):\n",
    "        pts = pw_estimates[f'breakpoint{i + 1}']['estimate']\n",
    "        idx_closest = int(np.argmin(np.abs(np.log10(n) - pts)))\n",
    "        y_ajustado, coeficientes = ajuste_polinomial(np.log10(n)[past_idx:idx_closest+1], np.log10(F)[past_idx:idx_closest+1], grado=1)\n",
    "        derivadas = np.concatenate((derivadas,evaluar_derivada(coeficientes, np.log10(n)[past_idx:idx_closest+1])))\n",
    "        y_ajustados = np.concatenate((y_ajustados,y_ajustado))    \n",
    "        past_idx = idx_closest+1\n",
    "    # ultimo segmento\n",
    "    y_ajustado, coeficientes = ajuste_polinomial(np.log10(n)[past_idx:], np.log10(F)[past_idx:], grado=1)\n",
    "    derivadas = np.concatenate((derivadas,evaluar_derivada(coeficientes, np.log10(n)[past_idx:])))\n",
    "    y_ajustados = np.concatenate((y_ajustados,y_ajustado))    \n",
    "\n",
    "    # Derivadas para surrogados\n",
    "    derivadas_surr = np.zeros([N, len(np.log10(n_surr))])\n",
    "    for surr_index in range(N):\n",
    "        past_idx_surr = 0\n",
    "        for i in range(n_breakpoints):\n",
    "            pts_surr = pw_estimates[f'breakpoint{i + 1}']['estimate']\n",
    "            idx_closest_surr = np.argmin(np.abs(np.log10(n_surr) - pts_surr))\n",
    "            y_ajustado_surr, coeficientes_surr = ajuste_polinomial(\n",
    "                np.log10(n_surr)[past_idx_surr:idx_closest_surr+1],\n",
    "                flucts_surrogates[surr_index, past_idx_surr:idx_closest_surr+1],\n",
    "                grado=1\n",
    "            )\n",
    "            derivadas_surr[surr_index, past_idx_surr:idx_closest_surr+1] = evaluar_derivada(\n",
    "                coeficientes_surr, np.log10(n_surr)[past_idx_surr:idx_closest_surr+1]\n",
    "            )\n",
    "            past_idx_surr = idx_closest_surr+1\n",
    "\n",
    "        # último segmento del surrogate\n",
    "        y_ajustado_surr, coeficientes_surr = ajuste_polinomial(\n",
    "            np.log10(n_surr)[past_idx_surr:],\n",
    "            flucts_surrogates[surr_index, past_idx_surr:],\n",
    "            grado=1\n",
    "        )\n",
    "        derivadas_surr[surr_index, past_idx_surr:] = evaluar_derivada(\n",
    "            coeficientes_surr, np.log10(n_surr)[past_idx_surr:]\n",
    "        )\n",
    "\n",
    "    derivadas_surr_mean = np.mean(derivadas_surr, axis=0)\n",
    "    derivadas_surr_std = np.std(derivadas_surr, axis=0)\n",
    "\n",
    "    # GrAFICAR POLINOMIO\n",
    "    plt.plot(np.log10(n), y_ajustados, color='green', label=f'Ajuste polinomial (grado {n_breakpoints+1})')\n",
    "\n",
    "    plt.xlabel('log(s)', fontsize=14)\n",
    "    plt.ylabel('log(F(s))', fontsize=14)\n",
    "    plt.title('DFA', fontsize=14)\n",
    "    plt.legend(loc=0, fontsize=7)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.log10(n), derivadas, color='green', ls='',marker='.',label='Derivadas de la serie original')\n",
    "    plt.fill_between(\n",
    "        np.log10(n_surr), \n",
    "        derivadas_surr_mean - derivadas_surr_std, \n",
    "        derivadas_surr_mean + derivadas_surr_std, \n",
    "        color='blue', alpha=0.2, label='Surrogates: Mean ± Std'\n",
    "    )\n",
    "    plt.xlabel('log(s)', fontsize=14)\n",
    "    plt.ylabel(\"Derivada del polinomio\", fontsize=14)\n",
    "    plt.title(\"Derivadas del ajuste polinomial vs log(s)\", fontsize=14)\n",
    "    plt.legend(loc=0, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "    # CALCULAR INDICE NO LINEALIDAD\n",
    "    E = np.sum(np.abs(derivadas - derivadas_surr_mean) / derivadas_surr_std)\n",
    "    E_index = E/len(derivadas)\n",
    "    return E_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for composer in composers_Hz_depurado_v2.keys():\n",
    "    xi_index = np.zeros((datos_composers_Hz_depurado_v2[composer]['# Piezas'], 2))\n",
    "    for i, serie in enumerate(composers_Hz_depurado_v2[composer].keys()): \n",
    "        # if int(serie.split('_')[1]) < 413:\n",
    "        #     print(serie)\n",
    "        #     continue\n",
    "        subject = composers_Hz_depurado_v2[composer][serie]\n",
    "        xi_index[i,0] = int(serie.split('_')[1])\n",
    "        xi_index[i,1] = main(subject, 'MDFA')\n",
    "    np.save('xi_index2/'+str(datos_composers_Hz_depurado_v2[composer]['Birth_year'])+'_'+str(composer)+'_xi.npy', xi_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
